# my_llm.py
import os
from typing import Optional
from openai import OpenAI
# from hello_agents import HelloAgentsLLM
from typing import List, Dict

class HelloAgentsLLM:
    """
    ä¸ºæœ¬ä¹¦ "Hello Agents" å®šåˆ¶çš„LLMå®¢æˆ·ç«¯ã€‚
    å®ƒç”¨äºè°ƒç”¨ä»»ä½•å…¼å®¹OpenAIæ¥å£çš„æœåŠ¡ï¼Œå¹¶é»˜è®¤ä½¿ç”¨æµå¼å“åº”ã€‚
    """
    def __init__(self, model: str = None, apiKey: str = None, baseUrl: str = None, timeout: int = None):
        """
        åˆå§‹åŒ–å®¢æˆ·ç«¯ã€‚ä¼˜å…ˆä½¿ç”¨ä¼ å…¥å‚æ•°ï¼Œå¦‚æœæœªæä¾›ï¼Œåˆ™ä»ç¯å¢ƒå˜é‡åŠ è½½ã€‚
        """
        self.model = model or os.getenv("LLM_MODEL_ID")
        apiKey = apiKey or os.getenv("LLM_API_KEY")
        baseUrl = baseUrl or os.getenv("LLM_BASE_URL")
        timeout = timeout or int(os.getenv("LLM_TIMEOUT", 60))
        
        if not all([self.model, apiKey, baseUrl]):
            raise ValueError("æ¨¡å‹IDã€APIå¯†é’¥å’ŒæœåŠ¡åœ°å€å¿…é¡»è¢«æä¾›æˆ–åœ¨.envæ–‡ä»¶ä¸­å®šä¹‰ã€‚")

        self.client = OpenAI(api_key=apiKey, base_url=baseUrl, timeout=timeout)

    def think(self, messages: List[Dict[str, str]], temperature: float = 0) -> str:
        """
        è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ€è€ƒï¼Œå¹¶è¿”å›å…¶å“åº”ã€‚
        """
        print(f"ğŸ§  æ­£åœ¨è°ƒç”¨ {self.model} æ¨¡å‹...")
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature,
                stream=True,
            )
            
            # å¤„ç†æµå¼å“åº”
            print("âœ… å¤§è¯­è¨€æ¨¡å‹å“åº”æˆåŠŸ:")
            collected_content = []
            for chunk in response:
                content = chunk.choices[0].delta.content or ""
                print(content, end="", flush=True)
                collected_content.append(content)
            print()  # åœ¨æµå¼è¾“å‡ºç»“æŸåæ¢è¡Œ
            return "".join(collected_content)

        except Exception as e:
            print(f"âŒ è°ƒç”¨LLM APIæ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return None
        
class MyLLM(HelloAgentsLLM):
    """
    ä¸€ä¸ªè‡ªå®šä¹‰çš„LLMå®¢æˆ·ç«¯ï¼Œé€šè¿‡ç»§æ‰¿å¢åŠ äº†å¯¹ModelScopeçš„æ”¯æŒã€‚
    """
    def __init__(
        self, 
        model: Optional[str] = None, 
        api_key: Optional[str] = None, 
        base_url: Optional[str] = None, 
        provider: Optional[str] = "auto", 
        **kwargs
    ):
        # æ£€æŸ¥provideræ˜¯å¦ä¸ºæˆ‘ä»¬æƒ³å¤„ç†çš„'modelscope'
        if provider == "modelscope":
            print("æ­£åœ¨ä½¿ç”¨è‡ªå®šä¹‰çš„ ModelScope Provider")
            self.provider = "modelscope"
            
            # è§£æ ModelScope çš„å‡­è¯
            self.api_key = api_key or os.getenv("MODELSCOPE_API_KEY")
            self.base_url = base_url or "https://api-inference.modelscope.cn/v1/"
            
            # éªŒè¯å‡­è¯æ˜¯å¦å­˜åœ¨
            if not self.api_key:
                raise ValueError("ModelScope API key not found. Please set MODELSCOPE_API_KEY environment variable.")

            # è®¾ç½®é»˜è®¤æ¨¡å‹å’Œå…¶ä»–å‚æ•°
            self.model = model or os.getenv("LLM_MODEL_ID") or "Qwen/Qwen2.5-VL-72B-Instruct"
            self.temperature = kwargs.get('temperature', 0.7)
            self.max_tokens = kwargs.get('max_tokens')
            self.timeout = kwargs.get('timeout', 60)
            
            # ä½¿ç”¨è·å–çš„å‚æ•°åˆ›å»ºOpenAIå®¢æˆ·ç«¯å®ä¾‹
            self._client = OpenAI(api_key=self.api_key, base_url=self.base_url, timeout=self.timeout)

        else:
            # å¦‚æœä¸æ˜¯ modelscope, åˆ™å®Œå…¨ä½¿ç”¨çˆ¶ç±»çš„åŸå§‹é€»è¾‘æ¥å¤„ç†
            super().__init__(model=model, api_key=api_key, base_url=base_url, provider=provider, **kwargs)

